{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:50px;\">Book Crossing Data Cleansing for Recommendations</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:25pt;\"> Introduction</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">In this notebook, I will use some techniques to perform Data Exploration and Cleansing on the  <a href=\"http://www2.informatik.uni-freiburg.de/~cziegler/BX/\">Book-Crossing Dataset</a> collected by Cai-Nicolas Ziegler. In doing so, I aim to gain some intuition about the data.<br>The most used weapon of choice for this notebook will be <a href='https://pandas.pydata.org/'>Pandas library.</a> </p>\n",
    "\n",
    " <img src=\"Data/images/data-cleansing-1.jpg\" width=\"300\" height=\"300\"> \n",
    "<p></p>\n",
    "<p style=\"color:red;font-size:15pt;\">The dataset consists of three files.</p>\n",
    " <ul style=\"font-size:20px\">\n",
    "  <li><em><b>BX_Users.csv</b></em> containing data from 278858 users (User Id, Location, Age).</li>\n",
    "  <li><em><b>BX_Books.csv</b></em> has information for 271379 books including Title, Author, Year of Publication, Publisher and Cover images.</li>\n",
    "  <li><em><b>BX_Book_Ratings.csv.</b></em> holding 1149780 ratings from the users.</li>\n",
    "</ul>\n",
    "<hr>\n",
    "<p style=\"font-size:15pt\">So, without further ado, let's import the necessary libraries and the data.</p>\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "import tqdm   #to visualize loops' progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">We downloaded the data in csv format so we are going to use <i>pandas.read_csv()</i> to create three DataFrames.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare the path variables\n",
    "root_path = os.getcwd()\n",
    "data_dir = os.path.join(root_path,'Data','datasets')\n",
    "new_data_dir = os.path.join(root_path,'Data','datasets','new datasets')\n",
    "if not os.path.isdir(new_data_dir):\n",
    "    os.mkdir(new_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data from csv\n",
    "# read Users\n",
    "u_cols = ['user_id', 'location', 'age']\n",
    "users = pd.read_csv(os.path.join(data_dir,'BX_Users.csv'), sep=';', names=u_cols, encoding='latin-1',low_memory=False)\n",
    "\n",
    "# read Books/items\n",
    "i_cols = ['isbn', 'book_title' ,'book_author','year_of_publication', 'publisher', 'img_s', 'img_m', 'img_l']\n",
    "items = pd.read_csv(os.path.join(data_dir,'BX_Books.csv'), sep=';', names=i_cols, encoding='latin-1',low_memory=False)\n",
    "\n",
    "# read Ratings\n",
    "r_cols = ['user_id', 'isbn', 'rating']\n",
    "ratings = pd.read_csv(os.path.join(data_dir,'BX_Book_Ratings.csv'), sep=';', names=r_cols, encoding='latin-1',low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>\n",
    "<p style=\"font-size:15pt\">Now let's print the 3 first elements of each dataframe we created</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__________________________________Users__________________________________')\n",
    "print(users.head(3))\n",
    "print(\"\\n\")\n",
    "print('__________________________________Items__________________________________')\n",
    "print(items.head(3))\n",
    "print(\"\\n\")\n",
    "print('_________________________________Ratings_________________________________')\n",
    "print(ratings.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p><p style=\"font-size:15pt\">As we can see the first line of each Dataframe (Df) contains the column names. We should remove them and then reset the index of the Dataframes. <br><br>Moreover, Df <b>\"Items\"</b> have 3 columns of image URLs. We are not going to need them so let's get rid of them.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.loc[1:]\n",
    "users.reset_index(drop=True, inplace=True)\n",
    "\n",
    "items = items.loc[1:]\n",
    "items.reset_index(drop=True, inplace=True)\n",
    "items.drop(['img_s','img_m','img_l'], axis=1, inplace=True)\n",
    "\n",
    "ratings = ratings.loc[1:]\n",
    "ratings.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('__________________________________Users__________________________________')\n",
    "print(users.head(5))\n",
    "print(\"\\n\")\n",
    "print('__________________________________Items__________________________________')\n",
    "print(items.head(5))\n",
    "print(\"\\n\")\n",
    "print('_________________________________Ratings_________________________________')\n",
    "print(ratings.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p><p style=\"font-size:15pt\">Now let's use DataFrame.describe() function to see what else we can find about our data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users.describe(),\"\\n\"*2)\n",
    "print(items.describe(),\"\\n\"*2)\n",
    "print(ratings.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">From the above, we can see that\n",
    "<ul style=\"font-size:12pt\">\n",
    "<li>Age is not numerical. We'll have to convert it to <em>int</em></li>\n",
    "<li>Year of publication is also not numerical. We'll have to convert it to <em>int</em> as well</li>\n",
    "<li>I also guess that there are some duplicate books in there.</li>\n",
    "<li>We have <em>ratings</em> for 340556 different book titles. This is more than the actual books in \"items\" Df</li>\n",
    "<li>The <em>ratings</em> come from 105283 different <em>users</em></li> \n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:12pt\"> </p>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:25pt;\"> Users DataFrame</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Now, let's take a closer look at the <em>\"users\"</em> Df.</p>\n",
    "<p style=\"font-size:15pt\"> Let's convert users.user_id to 'int' (we'll need that for later) and users.age to 'float' ('int' will be a good solution as well but since there are numerous NaN values we have to go with 'float' first.) Then let's call df.describe() again.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.age = users.age.astype(float)\n",
    "users.user_id = users.user_id.astype(int)\n",
    "users.describe(include=[object, int, float])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><h2 style=\"font-size:20pt;\"> users.age</h2>\n",
    "\n",
    "<p style=\"font-size:15pt\"> Let's take a closer look at \"users.age\" Series. As we can see from above it has a mean value around 34.75 and a deviation of 14.43. Next, we shall investigate how many NaN values there are in the Series and how many values are \"not so logical\" (> 5 or < 99).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NaN values in age:\", users.age[users.age.isna()].count())\n",
    "print(\"Users with 5 > age & age > 99 :\",users.loc[(users.age>99) | (users.age<5),'age'].count())\n",
    "users.loc[(users.age>99) | (users.age<5),'age'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\"><b>Wow</b>, 110762 missing values in <em>users.age</em> and another 1255 users have <i>age</i> values that I think are invalid. <br></br>Since we really don't like NaN values in datasets we will replace them with a value. This value can be the mean age of the users that registered their age or it can be based on some values of the dataset. For example, we can use the location of the user and use the average age of the population of this location, or we can use the average age of the population using PCs on that location since this dataset was collected from a website. We can even use an ML algorithm to determine the missing values. The easiest way  would be to use the mean.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.age.fillna(users.age.mean()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt\">But if we do that the deviation changes a lot, 3.1 units as shown above. And since I don't want to create a huge group of 112010 users with the same age, I am going to try something different.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a normal disgtribution pd.Series to fill Nan values with\n",
    "temp_age_series = pd.Series(np.random.normal(loc=users.age.mean(), scale=users.age.std(), size=users.user_id[users.age.isna()].count()))\n",
    "print(\"Statistics of values in \\'users.age\\'\\n\",users.age.describe(),\"\\n\")\n",
    "print(\"Statistics of values we are going to use to fill NaN \\n\",temp_age_series.describe(),\"\\n\")\n",
    "print(\"Negative values in \\'temp_age_seires\\':\", temp_age_series[temp_age_series<0].count(),\"\\n\")\n",
    "print(\"As we can see the destribution doesnt change a lot. There are some negative values thought (around 600 of them).\\n\")\n",
    "\n",
    "# take the abs value of temp_age_series\n",
    "pos_age_series=np.abs(temp_age_series)\n",
    "\n",
    "# sort users Df so as NaN values in age to be first and reset index to match with index of pos_age_series. Then use fillna()\n",
    "users = users.sort_values('age',na_position='first').reset_index(drop=True)\n",
    "users.age.fillna(pos_age_series, inplace = True)  \n",
    "\n",
    "# replace values < 5 with the mean(). Round values and convert them to int. \n",
    "users.loc[users.age<5, 'age'] = users.age.mean()\n",
    "users.age = users.age.round().astype(int)\n",
    "#Sort users based on user_id so as to be the same as before\n",
    "users = users.sort_values('user_id').reset_index(drop=True)\n",
    "print(users.age.describe(),\"\\n\")\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><p style=\"font-size:15pt\">Finally, we have to decide if we are going to let users.age as a numerical (quantitative) data or if we should change it to categorical (qualitative). If we choose \"numerical\" we add a continuity to the data. It might, for example, show that people read more books or rate them lower as they age. On the other hand, if we choose to change it to categorical then it will be better to talk about \"age group\" and we will treat the users of each group as a class.<br><br>I am going to leave it as is for now (numerical). In case you like to make it categorical all you have to do is to change each users age with a group (e.g {if (users.age>5)&(users.age< 15) then '5-15'} and so on ) and then use pd.get_dummies() to get the one-hot encoding of it.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><h2 style=\"font-size:20pt;\"> users.location</h2>\n",
    "<p style=\"font-size:15pt\">Continuing on with the <i>\"users.location\"</i> Series. It is formatted as a long string containing three comma-separated instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><p style=\"font-size:15pt\">I believe that it would be more insightful if those strings where separated. So, I suggest we should split the <i>\"users.location\"</i> in three different columns: <i>\"city\"</i>, <i>\"state\"</i> and <i>\"country\"</i> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_split=users.location.str.split(', ', n=2, expand=True)\n",
    "location_split.columns=['city', 'state', 'country']\n",
    "location_split.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">Having done that, we can easily detect invalid or NaN values. There are 2884 users with <em>\"state\" == ','</em>  and <em>\"country\"== None</em> . We are going to replace those values with 'other'.<br>There are also some users with ' ', '\\\\n/a\\\\\"', 'n.a' or '*' as a \"state\" value, we'll replace those with 'n/a'</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_split.loc[location_split.state==',', ['state', 'country']] = 'other'\n",
    "location_split.loc[location_split.country==',', ['country']] = 'other'\n",
    "location_split.loc[(location_split.state=='\\\\n/a\\\\\"') | (location_split.state=='') | (location_split.state=='*') | (location_split.state=='n.a'), ['state']] = 'n/a'\n",
    "location_split.state.fillna('other', inplace=True)\n",
    "location_split.fillna('n/a', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">All those different values would increase the dimensionality tremendously if we were to use One-Hot encoding. In an attempt to tackle this we can start replacing not so frequent values with 'other'. But that will lead us to lose quite a lot of data (depending on the \"keep threshold\"). And even with that, the resulting One-Hot encoding columns will be equal to the [No of unique strings in \"location_split\" = 40864 (after the previous operations)].\n",
    "<br><br>Instead, I suggest we transform all those locations to vectors (Word embeddings). To do this, we will train a <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2Vec model</a> and we will use the location values as input. In order to do this we will use the Gensim library implementation of the <a href=\"https://radimrehurek.com/gensim/models/word2vec.html\">model</a>. In this way, we will be able to replace the three columns with <b>3*n</b> new columns, where <b>n</b> is the size of the Word2Vec model Neural Network.\n",
    "<br>This might not be the conventional usage of a word embedding model but excect for keeping the dimensions low, it also helps in case we want to classify the users based on location.</p> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_location_df = pd.concat([location_split.city, location_split.state,  location_split.country, location_split.state, location_split.city, location_split.country, location_split.city], axis=1)\n",
    "location_list = temp_location_df.fillna('n/a').values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">You can change the parameters of the model but I think the selected ones work well.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "model = gensim.models.Word2Vec(location_list, size= n, window=3, min_count=1, workers=4)\n",
    "print ('UK is to Milton Keynes what Greece is to : ')\n",
    "model.wv.most_similar(positive=['greece','united kingdom'], negative=['milton keynes'], topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:13pt;\">With some of them being either cities or states of Greece.</p>\n",
    "<p style=\"font-size:15pt;\">The cell below constructs the 'location_vec' Df where \"city, \"state\" and \"country\" are represented with their respective vectors calculated with Word2Vec model.</p>\n",
    "<p style=\"font-size:13pt; color:green\">The operations that follow may require a substantial amount of time. If you choose to skip it, it will not affect the rest of the notebook.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rightchoice=['1','2']\n",
    "choice = input(\"Choose \\'1\\' to skip this step or \\'2\\' to construct the \\'location_vec\\' DataFrame.\")\n",
    "while choice not in rightchoice:\n",
    "    choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "if choice=='1':\n",
    "    print ('Skipping operations')\n",
    "else:\n",
    "    zipp = list(zip(model.wv.index2word, model.wv.syn0))\n",
    "    vectors = np.zeros((location_split.shape[0],3*n))\n",
    "    for i in tqdm.tqdm_notebook(range(location_split.shape[0])):\n",
    "        vectors[i, 0:20] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'city']]\n",
    "        vectors[i,20:40] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'state']]\n",
    "        vectors[i,40:60] = [j[1][0] for j in zipp if j[0] == location_split.loc[i, 'country']]\n",
    "    col=[]\n",
    "    for i in range(20):\n",
    "        col.append('city_'+ str(i))\n",
    "    for i in range(20):\n",
    "        col.append('state_'+ str(i))\n",
    "    for i in range(20):\n",
    "        col.append('country_'+ str(i))\n",
    "\n",
    "    location_vec = pd.DataFrame(vectors, columns = col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> users_new</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Finally, create a new DataFrame named \"users_new\". The new will contain</p> \n",
    "<ul style=\"font-size:13pt;\">\n",
    "the \"users.id\", <br>the \"location_vec\" if the Df exist or the \"location_spit\" otherwise <br>and the \"users.age\".\n",
    "</ul>\n",
    "<p style=\"font-size:12pt; color:green\">Replace \"users.age\" with pd.get_dummies(users.age) if you chose age to be categorical.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'location_vec' in globals():\n",
    "    users_new = pd.concat([users.user_id, location_vec , users.age], axis=1)    \n",
    "else:\n",
    "    users_new = pd.concat([users.user_id, location_split , users.age], axis=1)\n",
    "users_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">One last thing I would like to investigate in \"users\" before we move on to the next dataframe is the number of users that have rated at least one item. So...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(users_new[users_new.user_id.isin(ratings.user_id)].user_id.count(),'users have submited at least one review')\n",
    "print(users_new[~users_new.user_id.isin(ratings.user_id)].user_id.count(), 'users have not submited any review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:25pt;\"> Items DataFrame</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">The next DataFrame we will take a look at is <em>\"items\"</em> Df. As mentioned before it consists of the books' <em>isbn, title, author, year of publication and publisher</em>.<br><br> It would be a good start to convert the values in \"year_of_publication\" to int. When we try that we get the following error.\n",
    "<img src=\"Data/images/items_as(int)_error.jpg\" width=\"900\" height=\"150\">\n",
    "That indicates that some values got mixed up. I opened the csv in a text editor and corrected the few entries that were messed up and stored the file as \"BX_Books_correct.csv\". It turns out that most of the errors were caused by <em style=\"color:green\">\"&\"</em> character been written as <em style=\"color:red\">\"&</em><em style=\"font-size:4px;\"> </em><em style=\"color:red\">amp;\"</em> with the semicolon causing a string split at the wrong position of the csv file. There were also some other entries that contained semicolons at the \"book_title\" or the \"book_author\".</p> \n",
    "\n",
    "<p style=\"font-size:15pt;\">So, let's import the corrected csv.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pd.read_csv(os.path.join(data_dir,'BX_Books_correct.csv'), sep=';', names=i_cols, encoding='latin-1',low_memory=False)\n",
    "items = items.loc[1:]\n",
    "items.reset_index(drop=True, inplace=True)\n",
    "items.drop(['img_s','img_m','img_l'], axis=1, inplace=True)\n",
    "items.year_of_publication = items.year_of_publication.astype(int)\n",
    "items.describe(include =[object, int])\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">From the above we can see that : \n",
    "<ul style=\"font-size:12pt;\">\n",
    "<li> The \"isbn\" column has only unique values, as expected.</li>\n",
    "<li> There are duplicates in the \"book_titles\". Some might be indeed duplicate entries but some are just books with the same title. </li>\n",
    "<li>Agatha Christie is the most frequently appearing author. </li>\n",
    "<li>The min value in \"year_of publication\" is <b>0</b> and the max <b>2050 (?!?)</b>.</li>\n",
    "<li>There is one missing value in the \"book_author\" column and two in the \"publisher\".</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> Replace NaN and incorrect \"year_of_publication\" values</h2>\n",
    "<p style=\"font-size:15pt;\">Starting with replacing the missing values, since the number of them is so small I would prefer to replace them with the correct values rather than with \"other\" for example. Those items are:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Items with NaN values in \\\"book_author\\\": \\n\", items.isbn[items.book_author.isna()],\"\\n\")\n",
    "print (\"Items values in \\\"publisher\\\": \\n\", items.isbn[items.publisher.isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">A search for the item missing the \"book_author\" yields an item with no author. The search for the books with no publishers shows that they both have the same publisher. So the following lines of code fill NaN values with the correct ones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.loc[187701,'book_author'] = \"n/a\"\n",
    "items.loc[[128897, 129044],'publisher'] = \"NovelBooks, Inc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">Next, we should search for the incorrect \"year_of_publication\" values. This dataset was created in 2004 so it would be logical to assume that values greater than that should be replaced. But then I took a closer look at some items and they were published in 2005 so, I guess, the dataset was updated. I am going to use 2010 as an upper limit and investigate further for the lower limit.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Items with (year_of_publication > 2010):', items.year_of_publication[items.year_of_publication>2010].count(),'\\n')\n",
    "print('value_counts of items with (year_of_publication < 1500): \\n', items.year_of_publication[items.year_of_publication<1500].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.loc[(items.year_of_publication>2010)|(items.year_of_publication<1000),'year_of_publication'] = np.nan\n",
    "print(items.describe(),'\\n')\n",
    "print(items.year_of_publication.fillna(round(items.year_of_publication.mean())).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The mean and the std don't change that much so I will use the mean as a replacement. If you want, you can try the method used in <em>\"users.age\"</em> or something else...</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items.year_of_publication.fillna(round(items.year_of_publication.mean()),inplace=True)\n",
    "items.year_of_publication = items.year_of_publication.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:20pt;\"> Dealing with duplicate entries</h2>\n",
    "<p style=\"font-size:15pt;\">Moving on to duplicate detection of items entries, we saw before that there are 29225 duplicate book titles. That's counting only the ones that are exactly the same. Then again there might be some books with the exact same title but they might not be written by the same author, meaning <b>not</b> the same book (except if the author's name is written a bit differently so...). Now let's investigate how many items have the same title and author. They will be duplicate entries with great probability.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[(items.duplicated(['book_title', 'book_author'], keep=False))].describe(include=[object]))\n",
    "print(items[(items.duplicated(['book_title', 'book_author'], keep='first'))].describe(include=[object]))\n",
    "print(items[(items.duplicated(['book_title', 'book_author']))].book_author.value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">So, there are 35921 books with the same title and author (possibly different editions). If our goal is to build a <b>Book Recommender</b> then we care much more about the title rather than the specific editions. At least that's how I see it. That's why I believe we should delete all duplicates. From those 35921, if we keep the first occurrence, there will be 20175 redundant books. <br>But we must be <b>careful</b>! We have to transfer all the ratings of the deleted titles to the titles we will keep. We will do this later when we will process the <em>\"ratings\" Df</em>. For now let's create a <em>\"items_wo_duplicates\" Df</em>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_wo_duplicates = items.drop_duplicates(['book_title', 'book_author'])\n",
    "items_wo_duplicates.describe(include=[object,int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The new \"items\" Df now has 9050 duplicate books but none of them is also from the same author. Agatha Christie is no more the most frequent author but William Shakespeare took her place.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:25pt;\">Ratings DataFrame</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">The last DataFrame is <em>\"ratings\"</em> and consists of three columns. These are <em>\"user_id\", \"isbn\" and \"rating\".</em> <br>The ratings of each user are either <b>explicit</b> (1-10) meaning the user rated the item, or <b>implicit</b> (0) meaning the result of observed behavior.<br>First things first. As we saw earlier there are ratings which do not correspont to items in our dataset. We don't need those since we cannot recommend those items. We will create a \"ratings_new\" Df only with the ratings for the books in our dataset. And then we will convert the \"ratings.rating\" to 'int'. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_new = ratings[ratings.isbn.isin(items.isbn)]\n",
    "ratings_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_new.loc[:,'rating'] = ratings_new.rating.astype(int)\n",
    "print(ratings_new.rating.value_counts(sort=False))\n",
    "ratings_new.describe(include=[object,int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The thing that caught my eye immediately when looking at the \"ratings\" description is that more than 50% of all ratings are implicit.\n",
    "<br><br>But before we go any further we will have to transfer all ratings of duplicate titles and create the \"ratings_wo_duplicates\" Df. To do this you can choose <b>either</b> to import the '*.csv' file I have created <b>or</b> to reconstruct. <em>The latter might take a significant amount of time.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = input(\"Choose \\'1\\' to import the ratings_wo_duplicates file or \\'2\\' to construct it again \")\n",
    "while choice not in rightchoice:\n",
    "    choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "\n",
    "if choice == '1':\n",
    "    print('Importing \\'ratings_wo_duplicates.csv\\'')\n",
    "    ratings_wo_duplicates=pd.read_csv(os.path.join(data_dir,'ratings_wo_duplicates.csv'), sep=';', names=r_cols, encoding='latin-1', low_memory=False )\n",
    "    print('Done')\n",
    "elif choice == '2':\n",
    "    print('Constructing \\'ratings_wo_duplicates.csv\\'')\n",
    "    print('Please remember the number of processed and stored items incase the operation is interupted and you would like continue from there.')\n",
    "    \n",
    "    choice = input(\"Choose \\'1\\' to iterate through all items or \\'2\\' if this operation was interupted and you would like to continue from the last checkpoint.\")\n",
    "    while choice not in rightchoice:\n",
    "        choice = input(\"Wrong input. \\n Insert a number. Either 1 or 2\")\n",
    "    \n",
    "    if choice == '1':\n",
    "        nof = 0\n",
    "        ratings_wo_duplicates = ratings_new\n",
    "        count=0\n",
    "    else:\n",
    "        nof = int(input('Please insert the number of processed and stored items.'))\n",
    "        ratings_wo_duplicates=pd.read_csv(os.path.join(new_data_dir,'ratings_wo_duplicates.csv'), sep=';', names=r_cols, encoding='latin-1', low_memory=False )\n",
    "        count= nof\n",
    "    \n",
    "    # create a series with all the duplicates (including the first occurance) to iterate\n",
    "    temp=items[(items.duplicated(['book_title', 'book_author'],keep=False))][nof:]\n",
    "    \n",
    "    for t in tqdm.tqdm_notebook(temp['book_title']):\n",
    "        x = list( items[items['book_title']==t].isbn)\n",
    "        count+=1 \n",
    "        for i in range(1, len(x)):\n",
    "            #replace all entries in x list with x[0] (the isbn we kept in items_wo_duplicates)\n",
    "            ratings_wo_duplicates.loc[ratings_wo_duplicates.isbn==x[i],'isbn'] = x[0]\n",
    "\n",
    "        if count%2000==0:\n",
    "            ratings_wo_duplicates.to_csv(os.path.join(new_data_dir,'ratings_wo_duplicates.csv'),';', index=False)\n",
    "            print(count ,' duplicate items ratings processed and stored')\n",
    "            \n",
    "    ratings_wo_duplicates.to_csv(os.path.join(new_data_dir,'ratings_wo_duplicates.csv'),';', index=False)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "ratings_wo_duplicates = ratings_wo_duplicates.loc[1:]\n",
    "ratings_wo_duplicates.reset_index(drop=True, inplace=True)\n",
    "ratings_wo_duplicates.rating = ratings_wo_duplicates.rating.astype(int)\n",
    "print('\\nAnd to make sure that the procedure was carried out smoothly,')\n",
    "print('No of duplicates in \\\"ratings_wo_duplicates\\\" :',ratings_wo_duplicates.isbn[ratings_wo_duplicates.isbn.isin(items[items.duplicated(['book_title', 'book_author'])].isbn)].count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">The next move is to separate the explicit from the implicit ratings and place them into two Dataframes. Then create two new \"users\" Dfs containing the users who have rated an item either explicitly or implicitly accordingly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_expl = ratings_wo_duplicates[ratings_wo_duplicates.rating != 0]\n",
    "ratings_impl = ratings_wo_duplicates[ratings_wo_duplicates.rating == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ratings_expl.describe(include=[object,int]),'\\n')\n",
    "print(ratings_impl.describe(include=[object,int]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_w_ex_ratings = users_new[users_new.user_id.isin(ratings_expl.user_id)]\n",
    "users_w_im_ratings = users_new[users_new.user_id.isin(ratings_impl.user_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:15pt;\">I don't think there is much reason doing the same with the \"items\" dataframe but I have included the code anyway in the cell below.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_w_ratings = items_wo_duplicates[items_wo_duplicates.isbn.isin(ratings_wo_duplicates.isbn)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><h2 style=\"font-size:25pt;\">Final words</h2>\n",
    "\n",
    "<p style=\"font-size:15pt;\">Finaly, we end up with 7 new Dataframes.</p>\n",
    "<ul style=\"font-size:13pt;\">\n",
    "<li><b>users_new:</b> Users dataframe without NaN values and split location strings or word embeddings.</li>\n",
    "<li><b>users_w_ex_ratings:</b> Users who have contributed at least one explicit rating</li>\n",
    "<li><b>users_w_im_ratings:</b> Users who have contributed at least one implicit rating</li>\n",
    "<li><b>items_wo_duplicates:</b> Items dataframe without double entries or NaN values</li>\n",
    "<li><b>ratings_wo_duplicates:</b> Ratings corresponding to the items_wo_duplicates dataframe. All ratings kept and \"transferred\".</li>\n",
    "<li><b>ratings_expl:</b> Dataframe with the explicit ratings</li>\n",
    "<li><b>ratings_impl:</b> Dataframe with the implicit ratings</li>\n",
    "</ul>\n",
    "<br>\n",
    "<p style=\"font-size:15pt;\">The last step would be to save them for later use.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_new.to_csv(os.path.join(new_data_dir,'users_new.csv'),';', index=False)\n",
    "users_w_ex_ratings.to_csv(os.path.join(new_data_dir,'users_w_ex_ratings.csv'),';', index=False)\n",
    "users_w_im_ratings.to_csv(os.path.join(new_data_dir,'users_w_im_ratings.csv'),';', index=False)\n",
    "items_wo_duplicates.to_csv(os.path.join(new_data_dir,'items_wo_duplicates.csv'),';', index=False)\n",
    "ratings_wo_duplicates.to_csv(os.path.join(new_data_dir,'ratings_wo_duplicates.csv'),';', index=False)\n",
    "ratings_expl.to_csv(os.path.join(new_data_dir,'ratings_expl.csv'),';', index=False)\n",
    "ratings_impl.to_csv(os.path.join(new_data_dir,'ratings_impl.csv'),';', index=False)\n",
    "print(\"DONE!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>\n",
    "<p style=\"font-size:15pt;\">What we have to do in order to produce item recommendations would be to create the users/ratings matrix containing the ratings of each user to each rated item. Then, compute the distances or similarities between each user for a User-based collaborative filtering or between each item for an Item-based collaborative filtering... Of course, this dataset is huge so those matrices would be enormous and memory heavy so it would be better to do it in batches. \n",
    "<br><br>\n",
    "If you feel like droping me a line for any questions or suggestions don't hesitate to email me at <a href=\"mailto:ppelitaris@gmail.com?Subject=Consernig%20BookRecommendation%20Notebook\" target=\"_top\">ppelitaris@gmail.com</a>\n",
    "</p> \n",
    "<h2 style=\"font-size:25pt;\"align=right>To be continued...</h2>\n",
    "<pt>P.P.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
